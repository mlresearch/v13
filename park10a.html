 <head> 
  <link rel="alternate" type="application/rss+xml" href="http://jmlr.csail.mit.edu/jmlr.xml" title="JMLR RSS"> 
<link rel="stylesheet" type="text/css" href="http://jmlr.csail.mit.edu/style.css?%ffbw=kludge1%ff"> 
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
<style type="text/css"> 
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style> 
<script LANGUAGE='JavaScript'> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</script> 
 
 
<style> 
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
</head> 
 <body> 
 <div id="content">
<h2>Hierarchical Gaussian Process Regression</h2> 
<p><i><b>Sunho Park (POSTECH) and Seungjin Choi (POSTECH)</b></i>; 
JMLR W&P 13:95-110, 2010.</p> 
<h3>Abstract</h3> 
We address an approximation method for Gaussian process (GP) regression,
where we approximate covariance by a block matrix such that
diagonal blocks are calculated exactly while off-diagonal blocks are
approximated. Partitioning input data points, we present a two-layer
hierarchical model for GP regression, where prototypes of clusters in
the upper layer are involved for coarse modeling by a GP and data
points in each cluster in the lower layer are involved for fine modeling
by an individual GP whose prior mean is given by the corresponding
prototype and covariance is parameterized by data points in the
partition. In this hierarchical model, integrating out latent variables
in the upper layer leads to a block covariance matrix, where diagonal
blocks contain similarities between data points in the same partition
and off-diagonal blocks consist of approximate similarities calculated
using prototypes. This particular structure of the covariance matrix
divides the full GP into a pieces of manageable sub-problems whose
complexity scales with the number of data points in a partition. In
addition, our hierarchical GP regression (HGPR) is also useful for
cases where partitions of data reveal different characteristics. Experiments
on several benchmark datasets confirm the useful behavior of
our method.
</div> 
 <div id="fixed"> 
<br> 
<a align="right" href="http://www.jmlr.org" target=_top><img align="right" class="jmlr" src="http://jmlr.csail.mit.edu/jmlr.jpg" border="0"></a> 
<p><br><br> 
<p align="right"> <A href="http://www.jmlr.org/"> Home Page </A> 
 

<p align="right"> <A href="/papers"> Papers </A> 
 
<p align="right"> <A href="/author-info.html"> Submissions </A> 
 
<p align="right"> <A href="/news.html"> News </A> 
 
<p align="right"> <A href="/scope.html"> Scope </A> 
 
<p align="right"> <A href="/editorial-board.html"> Editorial Board </A> 
 

<p align="right"> <A href="/announcements.html"> Announcements </A> 
 
<p align="right"> <A href="/proceedings"> Proceedings </A> 
 
<p align="right"> <A href="/mloss">Open Source Software</A> 
 
<p align="right"> <A href="/search-jmlr.html"> Search </A> 
 
<p align="right"> <A href="/manudb"> Login </A></p> 
 

<br><br> 
<p align="right"> <A href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.csail.mit.edu/RSS.gif" class="rss" alt="RSS Feed"> 
</A> 
 
 
 
</div> 
 
</body>