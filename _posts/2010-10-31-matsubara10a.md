---
title: Adaptive Step-size Policy Gradients with Average Reward Metric
abstract: In this paper, we propose a novel adaptive step-size approach for policy
  gradient reinforcement learning. A new metric is defined for policy gradients that
  measures the effect of changes on average reward with respect to the policy parameters.
  Since the metric directly measures the effects on the average reward, the resulting
  policy gradient learning employs an adaptive step-size strategy that can effectively
  avoid falling into a stagnant phase from the complex structure of the average reward
  function with respect to the policy parameters. Two algorithms are derived with
  the metric as variants of ordinary and natural policy gradients. Their properties
  are compared with previously proposed policy gradients through numerical experiments
  with simple, but non-trivial, 3-state Markov Decision Processes (MDPs). We also
  show performance improvements over previous methods in on-line learning with more
  challenging 20-state MDPs.
pdf: http://proceedings.pmlr.press/matsubara10a/matsubara10a.pdf
layout: inproceedings
id: matsubara10a
month: 0
firstpage: 285
lastpage: 298
page: 285-298
origpdf: http://jmlr.org/proceedings/papers/v13/matsubara10a/matsubara10a.pdf
sections: 
author:
- given: Takamitsu
  family: Matsubara
- given: Tetsuro
  family: Morimura
- given: Jun
  family: Morimoto
date: 2010-10-31
address: Tokyo, Japan
publisher: PMLR
container-title: Proceedings of 2nd Asian Conference on Machine Learning
volume: '13'
genre: inproceedings
issued:
  date-parts:
  - 2010
  - 10
  - 31
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
