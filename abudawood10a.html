 <head> 
  <link rel="alternate" type="application/rss+xml" href="http://jmlr.csail.mit.edu/jmlr.xml" title="JMLR RSS"> 
<link rel="stylesheet" type="text/css" href="http://jmlr.csail.mit.edu/style.css?%ffbw=kludge1%ff"> 
<style>. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
<style type="text/css"> 
<!-- 
#fixed {
    position: absolute;
    top: 0;
    left: 0;
    width: 8em;
    height: 100%;
}
body > #fixed {
    position: fixed;
}
#content {
    margin-top: 1em;
    margin-left: 10em;
    margin-right: 0.5em;
}
img.jmlr {
    width: 7em;
}
img.rss {
    width: 2em;
}
-->
</style> 
<script LANGUAGE='JavaScript'> 
<!-- function GoAddress(user,machine) {
document.location = 'mailto:' + user + '@' + machine; } 
// -->
</script> 
 
 
<style> 
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style> 
</head> 
 <body> 
 <div id="content">
<h2>Exploiting the High Predictive Power of Multi-class Subgroups</h2> 
<p><i><b>Tarek Abudawood (University of Bristol) and Peter Flach (University of Bristol)</b></i>; 
JMLR W&P 13:177-192, 2010.</p> 
<h3>Abstract</h3> 
Subgroup discovery aims at finding subsets of a population whose
class distribution is significantly different from the overall distribution.
A number of multi-class subgroup discovery methods has been
previously investigated, proposed and implemented in the CN2-MSD
system. When a decision tree learner was applied using the induced
subgroups as features, it led to the construction of accurate and compact
predictive models, demonstrating the usefulness of the subgroups.
In this paper we show that, given a significant, sufficient and diverse
set of subgroups, no further learning phase is required to build a good
predictive model. Our systematic study bridges the gap between rule
learning and decision tree modelling by proposing a method which
uses the training information associated with the subgroups to form a
simple tree-based probability estimator and ranker, RankFree-MSD,
without the need for an additional learning phase. Furthermore, we
propose an efficient subgroup pruning algorithm, RankFree-Pruning,
that prunes unimportant subgroups from the subgroup tree in order
to reduce the number of subgroups and the size of the tree without
decreasing predictive performance. Despite the simplicity of our
approach we experimentally show that its predictive performance in
general is comparable to other decision tree and rule learners over 10
multi-class UCI data sets.
</div> 
 <div id="fixed"> 
<br> 
<a align="right" href="http://www.jmlr.org" target=_top><img align="right" class="jmlr" src="http://jmlr.csail.mit.edu/jmlr.jpg" border="0"></a> 
<p><br><br> 
<p align="right"> <A href="http://www.jmlr.org/"> Home Page </A> 
 

<p align="right"> <A href="/papers"> Papers </A> 
 
<p align="right"> <A href="/author-info.html"> Submissions </A> 
 
<p align="right"> <A href="/news.html"> News </A> 
 
<p align="right"> <A href="/scope.html"> Scope </A> 
 
<p align="right"> <A href="/editorial-board.html"> Editorial Board </A> 
 

<p align="right"> <A href="/announcements.html"> Announcements </A> 
 
<p align="right"> <A href="/proceedings"> Proceedings </A> 
 
<p align="right"> <A href="/mloss">Open Source Software</A> 
 
<p align="right"> <A href="/search-jmlr.html"> Search </A> 
 
<p align="right"> <A href="/manudb"> Login </A></p> 
 

<br><br> 
<p align="right"> <A href="http://jmlr.csail.mit.edu/jmlr.xml"> 
<img src="http://jmlr.csail.mit.edu/RSS.gif" class="rss" alt="RSS Feed"> 
</A> 
 
 
 
</div> 
 
</body>